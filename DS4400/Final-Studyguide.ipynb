{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [Ensemble Learning](#Ensemble-Learning)\n",
    "* [SVM](#SVM)\n",
    "* [Naive Bayes Classifier and Density Estimator](#Naive-Bayes-Classifier-and-Density-Estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Learning\n",
    "\n",
    "#### Bagging\n",
    "\n",
    "* Create k bootstrap samples (split your data k ways)\n",
    "* Train a distinct classifier on each split\n",
    "* Classify a testing point on majority vote/average\n",
    "\n",
    "OOB (out of bag) average error used instead of CV error.\n",
    "\n",
    "#### Random Forest\n",
    "\n",
    "* Bag training set, training n trees (same as normal bagging).\n",
    "* Also, at each tree split, a random sample of m features is chosen instead of all the features.\n",
    "\n",
    "#### AdaBoost\n",
    "\n",
    "1. Train 1 model with instance weights $w_{t}$\n",
    "2. Compute training error $\\epsilon_{t}$\n",
    "3. Choose $\\beta_{t} = \\frac{1}{2}ln(\\frac{1-\\epsilon_{t}}{\\epsilon_{t}})$\n",
    "4. Update instance weights $w_{t+1,i} = w_{t,i}exp(-\\beta_{t}y^{(i)}h_{t}(x^{(i)}))$. This makes it so that misclassified instances are considered more in the next model.\n",
    "5. Repeat 1-4 this T times. Final model is weighted combination of all these models.\n",
    "\n",
    "Ada boost works best with \"weak\" learners. In practice it does not overfit, and can be proven to reach 100% training accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM\n",
    "\n",
    "Line (2-dimensions): $\\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2} = 0$\n",
    "\n",
    "Hyperplane (d-dimensions): $\\theta_{0} + \\theta_{1}x_{1} + ... + \\theta_{d}x_{d} = 0$\n",
    "\n",
    "Looking for a hyperplane where:\n",
    "\n",
    "$\\theta_{0} + \\theta_{1}x^{(i)}_{1} + ... + \\theta_{d}x^{(i)}_{d} > 0\\text{ if }y^{(i)} = 1$\n",
    "\n",
    "$\\theta_{0} + \\theta_{1}x^{(i)}_{1} + ... + \\theta_{d}x^{(i)}_{d} < 0\\text{ if }y^{(i)} = -1$\n",
    "\n",
    "#### Classifier formulation 1\n",
    "\n",
    "*Note*: $y$ = 1 for the positive class, $y$ = -1 for the negative class.\n",
    "\n",
    "$\\text{max M}$\n",
    "\n",
    "$y^{i}(\\theta^{T}x) \\geq M \\text{ } \\forall i$\n",
    "\n",
    "$||\\theta||_{2} = 1$ &larr; Normalization constraint\n",
    "\n",
    "#### Classifier formulation 2\n",
    "\n",
    "$\\text{Min} ||\\theta||^{2}$\n",
    "\n",
    "$y^{i}(\\theta^{T}x^{(i)}) \\geq 1 \\text{ } \\forall i$\n",
    "\n",
    "* This is the easier formulation to optimize, and is equivalent.\n",
    "* Maximum margin classifier given by solution $\\theta$ to this optimization problem.\n",
    "\n",
    "#### Adding slack\n",
    "\n",
    "Maximum margin is not always the best. Using maximum margin is not resilient to outliers.\n",
    "\n",
    "$\\text{max M}$\n",
    "\n",
    "$y^{i}(\\theta^{T}x) \\geq M(1 - \\epsilon_{i}) \\text{ } \\forall i$\n",
    "\n",
    "$||\\theta||_{2} = 1$\n",
    "\n",
    "$\\epsilon_{i} \\geq 0, \\Sigma_{i}\\epsilon_{i} = C$\n",
    "\n",
    "C is the error budget hyperperameter.\n",
    "\n",
    "#### Adding slack (formulation 2)\n",
    "\n",
    "$\\text{Min } ||\\theta||^{2} + C\\text{ }\\Sigma_{i}\\epsilon_{i}$\n",
    "\n",
    "$y^{i}(\\theta^{T}x^{(i)}) \\geq 1 - \\epsilon_{i} \\text{ } \\forall i$\n",
    "\n",
    "$\\epsilon_{i} \\geq 0$\n",
    "\n",
    "Final classifier: $f(z) = \\theta_{0} + \\Sigma_{i}\\alpha_{i}<z,x^{(i)}>$. This is a linear combination of the inner product of the point and the support vectors.\n",
    "\n",
    "#### Properties\n",
    "\n",
    "* SVM is resilient to outliers.\n",
    "* Finds \"max margin classifier\".\n",
    "\n",
    "#### Hinge Loss\n",
    "\n",
    "$J(\\theta) = C\\text{ }\\Sigma^{n}_{i=1} max(0,1 - y^{(i)}h(x^{(i)})) + \\Sigma^{d}_{j=1}\\theta^{2}_{j}$\n",
    "\n",
    "#### Kernels\n",
    "\n",
    "A kernel can be subsituted for the linear combination fo the inner products of the support vectors.\n",
    "\n",
    "* Polynomial kernel of degree m\n",
    "    * $K(a,b) = (1 + \\Sigma^{d}_{i=1}a_{i}b_{i})^{m}$\n",
    "* Radial Basis Fuction (RBF) (gaussian kernel)\n",
    "    * $K(a,b) = exp(1-\\gamma\\Sigma^{d}_{i=0}(a_{i}-b_{i})^{2})$\n",
    "    \n",
    "Pros:\n",
    "\n",
    "* Non-linear features\n",
    "* More flexible decision boundary\n",
    "* Testing is computationally efficient\n",
    "    \n",
    "Cons:\n",
    "\n",
    "* Kernels need to be tuned (additional hyperparameters)\n",
    "* Training radial or polynomail kernels takes longer than linear SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier and Density Estimator\n",
    "\n",
    "#### Bayes' Rule\n",
    "\n",
    "$P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)}$\n",
    "\n",
    "#### Prior and Joint Probabilities\n",
    "\n",
    "**Prior probability**: Degree of belief without any other evidence\n",
    "\n",
    "**Joint probability**: Matrix of combined probabilities of a set of variables\n",
    "\n",
    "#### Density Estimation\n",
    "\n",
    "A density estimator learns a mapping from a set of attributes to a probability.\n",
    "\n",
    "Density estimator can tell you how likely a dataset is, assuming that all records were indepenently generated.\n",
    "\n",
    "$\\hat{P}(x_{1} \\land x_{2} \\land ... \\land x_{n} | M) = \\Pi^{n}_{i=1}\\hat{P}(x_{i}|M)$\n",
    "\n",
    "For large datasets, this usually will underflow (become really small), so log probabilities are used:\n",
    "\n",
    "$log \\hat{P}(x_{1} \\land x_{2} \\land ... \\land x_{n} | M) = \\Pi^{n}_{i=1}log\\hat{P}(x_{i}|M)$\n",
    "\n",
    "**Pros**\n",
    "\n",
    "* Density estimators can learn distribution of training data\n",
    "* Can compute probability for a record\n",
    "* Can do inference (predict likelihood of a record)\n",
    "\n",
    "**Cons**\n",
    "* Can overfit to the training data and not genrealize to test data\n",
    "* Curse of dimensionality\n",
    "\n",
    "Naive Bayes classifier will fix these cons.\n",
    "\n",
    "#### Naive Bayes Classifier\n",
    "\n",
    "Uses training data to estimate $P(X|Y) and P(Y)$, then uses Bayes' rule to infer $P(Y|X_{new})$\n",
    "\n",
    "Need to assume that each feature is independent.\n",
    "\n",
    "Some probabilities can be 0 based on this: If there are 0 examples of label $y$ given feature $x_{i}=z$. Fix this with Laplace Smoothing.\n",
    "\n",
    "##### Laplace Smoothing\n",
    "\n",
    "Essentially, add 1 to each count so that no probability can be 0 (only close to 0).\n",
    "\n",
    "Naive Bayes classifier gives predictions, not probabilities, as the denominator of Bayes' rule is ignored.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
