{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [Ensemble Learning](#Ensemble-Learning)\n",
    "* [SVM](#SVM)\n",
    "* [Naive Bayes Classifier and Density Estimator](#Naive-Bayes-Classifier-and-Density-Estimator)\n",
    "* [Neural Networks](#Neural-Networks)\n",
    "* [Convolutional Neural Networks](#Convolutional-Neural-Networks)\n",
    "* [Backpropagation](#Backpropagation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Learning\n",
    "\n",
    "#### Bagging\n",
    "\n",
    "* Create k bootstrap samples (split your data k ways)\n",
    "* Train a distinct classifier on each split\n",
    "* Classify a testing point on majority vote/average\n",
    "\n",
    "OOB (out of bag) average error used instead of CV error.\n",
    "\n",
    "#### Random Forest\n",
    "\n",
    "* Bag training set, training n trees (same as normal bagging).\n",
    "* Also, at each tree split, a random sample of m features is chosen instead of all the features.\n",
    "\n",
    "#### AdaBoost\n",
    "\n",
    "1. Train 1 model with instance weights $w_{t}$\n",
    "2. Compute training error $\\epsilon_{t}$\n",
    "3. Choose $\\beta_{t} = \\frac{1}{2}ln(\\frac{1-\\epsilon_{t}}{\\epsilon_{t}})$\n",
    "4. Update instance weights $w_{t+1,i} = w_{t,i}exp(-\\beta_{t}y^{(i)}h_{t}(x^{(i)}))$. This makes it so that misclassified instances are considered more in the next model.\n",
    "5. Repeat 1-4 this T times. Final model is weighted combination of all these models.\n",
    "\n",
    "Ada boost works best with \"weak\" learners. In practice it does not overfit, and can be proven to reach 100% training accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM\n",
    "\n",
    "Line (2-dimensions): $\\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2} = 0$\n",
    "\n",
    "Hyperplane (d-dimensions): $\\theta_{0} + \\theta_{1}x_{1} + ... + \\theta_{d}x_{d} = 0$\n",
    "\n",
    "Looking for a hyperplane where:\n",
    "\n",
    "$\\theta_{0} + \\theta_{1}x^{(i)}_{1} + ... + \\theta_{d}x^{(i)}_{d} > 0\\text{ if }y^{(i)} = 1$\n",
    "\n",
    "$\\theta_{0} + \\theta_{1}x^{(i)}_{1} + ... + \\theta_{d}x^{(i)}_{d} < 0\\text{ if }y^{(i)} = -1$\n",
    "\n",
    "#### Classifier formulation 1\n",
    "\n",
    "*Note*: $y$ = 1 for the positive class, $y$ = -1 for the negative class.\n",
    "\n",
    "$\\text{max M}$\n",
    "\n",
    "$y^{i}(\\theta^{T}x) \\geq M \\text{ } \\forall i$\n",
    "\n",
    "$||\\theta||_{2} = 1$ &larr; Normalization constraint\n",
    "\n",
    "#### Classifier formulation 2\n",
    "\n",
    "$\\text{Min} ||\\theta||^{2}$\n",
    "\n",
    "$y^{i}(\\theta^{T}x^{(i)}) \\geq 1 \\text{ } \\forall i$\n",
    "\n",
    "* This is the easier formulation to optimize, and is equivalent.\n",
    "* Maximum margin classifier given by solution $\\theta$ to this optimization problem.\n",
    "\n",
    "#### Adding slack\n",
    "\n",
    "Maximum margin is not always the best. Using maximum margin is not resilient to outliers.\n",
    "\n",
    "$\\text{max M}$\n",
    "\n",
    "$y^{i}(\\theta^{T}x) \\geq M(1 - \\epsilon_{i}) \\text{ } \\forall i$\n",
    "\n",
    "$||\\theta||_{2} = 1$\n",
    "\n",
    "$\\epsilon_{i} \\geq 0, \\Sigma_{i}\\epsilon_{i} = C$\n",
    "\n",
    "C is the error budget hyperperameter.\n",
    "\n",
    "#### Adding slack (formulation 2)\n",
    "\n",
    "$\\text{Min } ||\\theta||^{2} + C\\text{ }\\Sigma_{i}\\epsilon_{i}$\n",
    "\n",
    "$y^{i}(\\theta^{T}x^{(i)}) \\geq 1 - \\epsilon_{i} \\text{ } \\forall i$\n",
    "\n",
    "$\\epsilon_{i} \\geq 0$\n",
    "\n",
    "Final classifier: $f(z) = \\theta_{0} + \\Sigma_{i}\\alpha_{i}<z,x^{(i)}>$. This is a linear combination of the inner product of the point and the support vectors.\n",
    "\n",
    "#### Properties\n",
    "\n",
    "* SVM is resilient to outliers.\n",
    "* Finds \"max margin classifier\".\n",
    "\n",
    "#### Hinge Loss\n",
    "\n",
    "$J(\\theta) = C\\text{ }\\Sigma^{n}_{i=1} max(0,1 - y^{(i)}h(x^{(i)})) + \\Sigma^{d}_{j=1}\\theta^{2}_{j}$\n",
    "\n",
    "#### Kernels\n",
    "\n",
    "A kernel can be subsituted for the linear combination fo the inner products of the support vectors.\n",
    "\n",
    "* Polynomial kernel of degree m\n",
    "    * $K(a,b) = (1 + \\Sigma^{d}_{i=1}a_{i}b_{i})^{m}$\n",
    "* Radial Basis Fuction (RBF) (gaussian kernel)\n",
    "    * $K(a,b) = exp(1-\\gamma\\Sigma^{d}_{i=0}(a_{i}-b_{i})^{2})$\n",
    "    \n",
    "Pros:\n",
    "\n",
    "* Non-linear features\n",
    "* More flexible decision boundary\n",
    "* Testing is computationally efficient\n",
    "    \n",
    "Cons:\n",
    "\n",
    "* Kernels need to be tuned (additional hyperparameters)\n",
    "* Training radial or polynomail kernels takes longer than linear SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier and Density Estimator\n",
    "\n",
    "#### Bayes' Rule\n",
    "\n",
    "$P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)}$\n",
    "\n",
    "#### Prior and Joint Probabilities\n",
    "\n",
    "**Prior probability**: Degree of belief without any other evidence\n",
    "\n",
    "**Joint probability**: Matrix of combined probabilities of a set of variables\n",
    "\n",
    "#### Density Estimation\n",
    "\n",
    "A density estimator learns a mapping from a set of attributes to a probability.\n",
    "\n",
    "Density estimator can tell you how likely a dataset is, assuming that all records were indepenently generated.\n",
    "\n",
    "$\\hat{P}(x_{1} \\land x_{2} \\land ... \\land x_{n} | M) = \\Pi^{n}_{i=1}\\hat{P}(x_{i}|M)$\n",
    "\n",
    "For large datasets, this usually will underflow (become really small), so log probabilities are used:\n",
    "\n",
    "$log \\hat{P}(x_{1} \\land x_{2} \\land ... \\land x_{n} | M) = \\Pi^{n}_{i=1}log\\hat{P}(x_{i}|M)$\n",
    "\n",
    "**Pros**\n",
    "\n",
    "* Density estimators can learn distribution of training data\n",
    "* Can compute probability for a record\n",
    "* Can do inference (predict likelihood of a record)\n",
    "\n",
    "**Cons**\n",
    "* Can overfit to the training data and not genrealize to test data\n",
    "* Curse of dimensionality\n",
    "\n",
    "Naive Bayes classifier will fix these cons.\n",
    "\n",
    "#### Naive Bayes Classifier\n",
    "\n",
    "Uses training data to estimate $P(X|Y)$ and $P(Y)$, then uses Bayes' rule to infer $P(Y|X_{new})$\n",
    "\n",
    "Need to assume that each feature is independent.\n",
    "\n",
    "Some probabilities can be 0 based on this: If there are 0 examples of label $y$ given feature $x_{i}=z$. Fix this with Laplace Smoothing.\n",
    "\n",
    "##### Laplace Smoothing\n",
    "\n",
    "Essentially, add 1 to each count so that no probability can be 0 (only close to 0).\n",
    "\n",
    "Naive Bayes classifier gives predictions, not probabilities, as the denominator of Bayes' rule is ignored.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks\n",
    "\n",
    "* Neural networks are made up of nodes connected by links.\n",
    "* Each link has an associated weight.\n",
    "* Each node has an input function (sum of inputs) and an activation function which produces an output.\n",
    "\n",
    "Neural networks use a \"bias unit\", which is set to 1. This is just like the bias unit in linear/logistic regression. A weight is then learned on that unit.\n",
    "\n",
    "<img src=\"images/neural-network-1.png\" style=\"width:400px\"/>\n",
    "\n",
    "#### Feed-Forward Networks\n",
    "\n",
    "Neurons from each layer connect to neurons in the next layer. This would be considered the \"basic\" neural network.\n",
    "\n",
    "To get a prediction from a feed forward network, inputs are fed to layer 0, then the weighted sums are applied on each node in the hidden layer. The hidden layer then applies the activation function, and this proceeds until the output layer is reached.\n",
    "\n",
    "<img src=\"images/neural-network-2.png\" style=\"width:400px\"/>\n",
    "\n",
    "#### Vectorization\n",
    "\n",
    "Vectorization can be used to speed up computations. Essentially, this uses linear algebra to compute many values at once. Example:\n",
    "\n",
    "$z^{[1]} = W^{[1]}x + b^{[1]}$ &rarr; produces the first hidden layer. $W^{[1]}$ is the weights for evey neuron in the hidden layer.\n",
    "\n",
    "#### Terminology\n",
    "\n",
    "* $a_{i}^{[j]}$ - Activation of unit i in hidden layer j\n",
    "* g - activation function\n",
    "* $W_{j}$ - Weight vector for hidden layer j. **NOTE** this seems incorrect, but is copied from slides. I believe it should be $W^{[j]}$\n",
    "* $b_{j}$ - Bias vector for hidden layer j. **See note above**\n",
    "\n",
    "$W$ and $b$ are the trainable parameters for the neural network.\n",
    "\n",
    "\n",
    "#### Softmax\n",
    "\n",
    "$\\sigma(z)_{j} = \\frac{e^{z_{j}}}{\\Sigma^{k}_{k=1}e^{z_{k}}}$\n",
    "\n",
    "Basically, creates an output for each class, and the highest output class is predicted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks\n",
    "\n",
    "Conv nets contain at least one convolutional layer.\n",
    "\n",
    "A convolution is an $n\\times m$ (but typically $n \\times n$) size filter, that is run over the input, computing dot products. Convolutions are followed by an activation function (typically ReLU)\n",
    "\n",
    "<img src=\"images/conv-net-1.png\" style=\"width:400px\"/>\n",
    "\n",
    "Note how above, a filter of size 5 applied to an image of size 32 produced a matrix of size 28. This is a general pattern. Output size = Input size - (conv size - 1), as long as stride = 1.\n",
    "\n",
    "#### Stride\n",
    "\n",
    "Stride is how far you move the convolution each time. Typically stride=1, but sometimes other strides are used.\n",
    "\n",
    "The general formula is: ((Input size - (conv size)) / stride) + 1. \n",
    "\n",
    "#### Padding\n",
    "\n",
    "Notice how this could result in a fractional output size! To fix this, padding is used. Essentially, you add a border of 0's to the input so that you get an integer output size.\n",
    "\n",
    "#### Max Pooling\n",
    "\n",
    "Max pooling is typically used after a convolutional layer.\n",
    "\n",
    "<img src=\"images/pool-1.png\" style=\"width:300px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "\n",
    "#### Parameter Initialization\n",
    "\n",
    "Initializing parameters with all 0s would make each unit learn the same thing. Instead, we randomly initialize.\n",
    "\n",
    "#### Computing Gradients\n",
    "\n",
    "<img src=\"images/backprop-1.png\" style=\"width:400px\"/>\n",
    "\n",
    "**Step 1**\n",
    "\n",
    "For output layer N, we have:\n",
    "\n",
    "You can think of $\\delta^{[N]}$ as the error at layer $N$\n",
    "\n",
    "Let's say our network has 4 layers. At layer 4, $\\delta^{[4]}=a^{[4]} - y$, as this is the difference between our prediction and the label.\n",
    "\n",
    "At the other layers we use the following formula:\n",
    "\n",
    "$\\delta^{[N]}=\\bigtriangledown_{z^{[N]}}L(\\hat{y},y)$\n",
    "\n",
    "\n",
    "For sigmoid function, this would be computed with the chain rule:\n",
    "\n",
    "$\\bigtriangledown_{z^{[N]}}L(\\hat{y},y)=\\bigtriangledown_{\\hat{y}}L(\\hat{y},y)\\circ(g^{[N]})'(z^{[N]})$\n",
    "\n",
    "**Step 2**\n",
    "\n",
    "For $l= N-1, N-2,...,1$, we have:\n",
    "\n",
    "$\\delta^{[l]} = (W^{[l+1]T}\\delta^{[l+1]})\\circ g'(z^{[l]})$\n",
    "\n",
    "For the sigmoid function, $g'(z^{[l]})=a^{[l]} \\circ (1 - a^{[l]\n",
    "\n",
    "Note that the gradient from the previous layer is used to compute the gradient for the current layer (going backwards)\n",
    "\n",
    "**Step 3**\n",
    "\n",
    "Compute the gradients at layer $l$ as:\n",
    "\n",
    "$\\bigtriangledown_{W^{[l}}J(W,B) = \\delta^{[l]}a^{[l-1]T}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
