{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [Ensemble Learning](#Ensemble-Learning)\n",
    "* [SVM](#SVM)\n",
    "* [Naive Bayes Classifier and Density Estimator](#Naive-Bayes-Classifier-and-Density-Estimator)\n",
    "* [Neural Networks](#Neural-Networks)\n",
    "* [Convolutional Neural Networks](#Convolutional-Neural-Networks)\n",
    "* [Backpropagation](#Backpropagation)\n",
    "* [Recurrent Neural Networks \"RNN\"](#Recurrent-Neural-Networks-\"RNN\")\n",
    "* [PCA](#PCA)\n",
    "* [Autoencoders](#Autoencoders)\n",
    "* [Clustering](#Clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Learning\n",
    "\n",
    "#### Bagging\n",
    "\n",
    "* Create k bootstrap samples (split your data k ways)\n",
    "* Train a distinct classifier on each split\n",
    "* Classify a testing point on majority vote/average\n",
    "\n",
    "OOB (out of bag) average error used instead of CV error.\n",
    "\n",
    "#### Random Forest\n",
    "\n",
    "* Bag training set, training n trees (same as normal bagging).\n",
    "* Also, at each tree split, a random sample of m features is chosen instead of all the features.\n",
    "\n",
    "#### AdaBoost\n",
    "\n",
    "1. Train 1 model with instance weights $w_{t}$\n",
    "2. Compute training error $\\epsilon_{t}$\n",
    "3. Choose $\\beta_{t} = \\frac{1}{2}ln(\\frac{1-\\epsilon_{t}}{\\epsilon_{t}})$\n",
    "4. Update instance weights $w_{t+1,i} = w_{t,i}exp(-\\beta_{t}y^{(i)}h_{t}(x^{(i)}))$. This makes it so that misclassified instances are considered more in the next model.\n",
    "5. Repeat 1-4 this T times. Final model is weighted combination of all these models.\n",
    "\n",
    "Ada boost works best with \"weak\" learners. In practice it does not overfit, and can be proven to reach 100% training accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM\n",
    "\n",
    "Line (2-dimensions): $\\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2} = 0$\n",
    "\n",
    "Hyperplane (d-dimensions): $\\theta_{0} + \\theta_{1}x_{1} + ... + \\theta_{d}x_{d} = 0$\n",
    "\n",
    "Looking for a hyperplane where:\n",
    "\n",
    "$\\theta_{0} + \\theta_{1}x^{(i)}_{1} + ... + \\theta_{d}x^{(i)}_{d} > 0\\text{ if }y^{(i)} = 1$\n",
    "\n",
    "$\\theta_{0} + \\theta_{1}x^{(i)}_{1} + ... + \\theta_{d}x^{(i)}_{d} < 0\\text{ if }y^{(i)} = -1$\n",
    "\n",
    "#### Classifier formulation 1\n",
    "\n",
    "*Note*: $y$ = 1 for the positive class, $y$ = -1 for the negative class.\n",
    "\n",
    "$\\text{max M}$\n",
    "\n",
    "$y^{i}(\\theta^{T}x) \\geq M \\text{ } \\forall i$\n",
    "\n",
    "$||\\theta||_{2} = 1$ &larr; Normalization constraint\n",
    "\n",
    "#### Classifier formulation 2\n",
    "\n",
    "$\\text{Min} ||\\theta||^{2}$\n",
    "\n",
    "$y^{i}(\\theta^{T}x^{(i)}) \\geq 1 \\text{ } \\forall i$\n",
    "\n",
    "* This is the easier formulation to optimize, and is equivalent.\n",
    "* Maximum margin classifier given by solution $\\theta$ to this optimization problem.\n",
    "\n",
    "#### Adding slack\n",
    "\n",
    "Maximum margin is not always the best. Using maximum margin is not resilient to outliers.\n",
    "\n",
    "$\\text{max M}$\n",
    "\n",
    "$y^{i}(\\theta^{T}x) \\geq M(1 - \\epsilon_{i}) \\text{ } \\forall i$\n",
    "\n",
    "$||\\theta||_{2} = 1$\n",
    "\n",
    "$\\epsilon_{i} \\geq 0, \\Sigma_{i}\\epsilon_{i} = C$\n",
    "\n",
    "C is the error budget hyperperameter.\n",
    "\n",
    "#### Adding slack (formulation 2)\n",
    "\n",
    "$\\text{Min } ||\\theta||^{2} + C\\text{ }\\Sigma_{i}\\epsilon_{i}$\n",
    "\n",
    "$y^{i}(\\theta^{T}x^{(i)}) \\geq 1 - \\epsilon_{i} \\text{ } \\forall i$\n",
    "\n",
    "$\\epsilon_{i} \\geq 0$\n",
    "\n",
    "Final classifier: $f(z) = \\theta_{0} + \\Sigma_{i}\\alpha_{i}<z,x^{(i)}>$. This is a linear combination of the inner product of the point and the support vectors.\n",
    "\n",
    "#### Properties\n",
    "\n",
    "* SVM is resilient to outliers.\n",
    "* Finds \"max margin classifier\".\n",
    "\n",
    "#### Hinge Loss\n",
    "\n",
    "$J(\\theta) = C\\text{ }\\Sigma^{n}_{i=1} max(0,1 - y^{(i)}h(x^{(i)})) + \\Sigma^{d}_{j=1}\\theta^{2}_{j}$\n",
    "\n",
    "#### Kernels\n",
    "\n",
    "A kernel can be subsituted for the linear combination fo the inner products of the support vectors.\n",
    "\n",
    "* Polynomial kernel of degree m\n",
    "    * $K(a,b) = (1 + \\Sigma^{d}_{i=1}a_{i}b_{i})^{m}$\n",
    "* Radial Basis Fuction (RBF) (gaussian kernel)\n",
    "    * $K(a,b) = exp(1-\\gamma\\Sigma^{d}_{i=0}(a_{i}-b_{i})^{2})$\n",
    "    \n",
    "Pros:\n",
    "\n",
    "* Non-linear features\n",
    "* More flexible decision boundary\n",
    "* Testing is computationally efficient\n",
    "    \n",
    "Cons:\n",
    "\n",
    "* Kernels need to be tuned (additional hyperparameters)\n",
    "* Training radial or polynomail kernels takes longer than linear SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier and Density Estimator\n",
    "\n",
    "#### Bayes' Rule\n",
    "\n",
    "$P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)}$\n",
    "\n",
    "#### Prior and Joint Probabilities\n",
    "\n",
    "**Prior probability**: Degree of belief without any other evidence\n",
    "\n",
    "**Joint probability**: Matrix of combined probabilities of a set of variables\n",
    "\n",
    "#### Density Estimation\n",
    "\n",
    "A density estimator learns a mapping from a set of attributes to a probability.\n",
    "\n",
    "Density estimator can tell you how likely a dataset is, assuming that all records were indepenently generated.\n",
    "\n",
    "$\\hat{P}(x_{1} \\land x_{2} \\land ... \\land x_{n} | M) = \\Pi^{n}_{i=1}\\hat{P}(x_{i}|M)$\n",
    "\n",
    "For large datasets, this usually will underflow (become really small), so log probabilities are used:\n",
    "\n",
    "$log \\hat{P}(x_{1} \\land x_{2} \\land ... \\land x_{n} | M) = \\Pi^{n}_{i=1}log\\hat{P}(x_{i}|M)$\n",
    "\n",
    "**Pros**\n",
    "\n",
    "* Density estimators can learn distribution of training data\n",
    "* Can compute probability for a record\n",
    "* Can do inference (predict likelihood of a record)\n",
    "\n",
    "**Cons**\n",
    "* Can overfit to the training data and not genrealize to test data\n",
    "* Curse of dimensionality\n",
    "\n",
    "Naive Bayes classifier will fix these cons.\n",
    "\n",
    "#### Naive Bayes Classifier\n",
    "\n",
    "Uses training data to estimate $P(X|Y)$ and $P(Y)$, then uses Bayes' rule to infer $P(Y|X_{new})$\n",
    "\n",
    "Need to assume that each feature is independent.\n",
    "\n",
    "Some probabilities can be 0 based on this: If there are 0 examples of label $y$ given feature $x_{i}=z$. Fix this with Laplace Smoothing.\n",
    "\n",
    "##### Laplace Smoothing\n",
    "\n",
    "Essentially, add 1 to each count so that no probability can be 0 (only close to 0).\n",
    "\n",
    "Naive Bayes classifier gives predictions, not probabilities, as the denominator of Bayes' rule is ignored.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks\n",
    "\n",
    "* Neural networks are made up of nodes connected by links.\n",
    "* Each link has an associated weight.\n",
    "* Each node has an input function (sum of inputs) and an activation function which produces an output.\n",
    "\n",
    "Neural networks use a \"bias unit\", which is set to 1. This is just like the bias unit in linear/logistic regression. A weight is then learned on that unit.\n",
    "\n",
    "<img src=\"images/neural-network-1.png\" style=\"width:400px\"/>\n",
    "\n",
    "#### Feed-Forward Networks\n",
    "\n",
    "Neurons from each layer connect to neurons in the next layer. This would be considered the \"basic\" neural network.\n",
    "\n",
    "To get a prediction from a feed forward network, inputs are fed to layer 0, then the weighted sums are applied on each node in the hidden layer. The hidden layer then applies the activation function, and this proceeds until the output layer is reached.\n",
    "\n",
    "<img src=\"images/neural-network-2.png\" style=\"width:400px\"/>\n",
    "\n",
    "#### Vectorization\n",
    "\n",
    "Vectorization can be used to speed up computations. Essentially, this uses linear algebra to compute many values at once. Example:\n",
    "\n",
    "$z^{[1]} = W^{[1]}x + b^{[1]}$ &rarr; produces the first hidden layer. $W^{[1]}$ is the weights for evey neuron in the hidden layer.\n",
    "\n",
    "#### Terminology\n",
    "\n",
    "* $a_{i}^{[j]}$ - Activation of unit i in hidden layer j\n",
    "* g - activation function\n",
    "* $W_{j}$ - Weight vector for hidden layer j. **NOTE** this seems incorrect, but is copied from slides. I believe it should be $W^{[j]}$\n",
    "* $b_{j}$ - Bias vector for hidden layer j. **See note above**\n",
    "\n",
    "$W$ and $b$ are the trainable parameters for the neural network.\n",
    "\n",
    "\n",
    "#### Softmax\n",
    "\n",
    "$\\sigma(z)_{j} = \\frac{e^{z_{j}}}{\\Sigma^{k}_{k=1}e^{z_{k}}}$\n",
    "\n",
    "Basically, creates an output for each class, and the highest output class is predicted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks\n",
    "\n",
    "Conv nets contain at least one convolutional layer.\n",
    "\n",
    "A convolution is an $n\\times m$ (but typically $n \\times n$) size filter, that is run over the input, computing dot products. Convolutions are followed by an activation function (typically ReLU)\n",
    "\n",
    "<img src=\"images/conv-net-1.png\" style=\"width:400px\"/>\n",
    "\n",
    "Note how above, a filter of size 5 applied to an image of size 32 produced a matrix of size 28. This is a general pattern. Output size = Input size - (conv size - 1), as long as stride = 1.\n",
    "\n",
    "#### Stride\n",
    "\n",
    "Stride is how far you move the convolution each time. Typically stride=1, but sometimes other strides are used.\n",
    "\n",
    "The general formula is: ((Input size - (conv size)) / stride) + 1. \n",
    "\n",
    "#### Padding\n",
    "\n",
    "Notice how this could result in a fractional output size! To fix this, padding is used. Essentially, you add a border of 0's to the input so that you get an integer output size.\n",
    "\n",
    "#### Max Pooling\n",
    "\n",
    "Max pooling is typically used after a convolutional layer.\n",
    "\n",
    "<img src=\"images/pool-1.png\" style=\"width:300px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "\n",
    "#### Parameter Initialization\n",
    "\n",
    "Initializing parameters with all 0s would make each unit learn the same thing. Instead, we randomly initialize.\n",
    "\n",
    "#### Computing Gradients\n",
    "\n",
    "<img src=\"images/backprop-1.png\" style=\"width:400px\"/>\n",
    "\n",
    "**Step 1**\n",
    "\n",
    "For output layer N, we have:\n",
    "\n",
    "You can think of $\\delta^{[N]}$ as the error at layer $N$\n",
    "\n",
    "Let's say our network has 4 layers. At layer 4, $\\delta^{[4]}=a^{[4]} - y$, as this is the difference between our prediction and the label.\n",
    "\n",
    "At the other layers we use the following formula:\n",
    "\n",
    "$\\delta^{[N]}=\\bigtriangledown_{z^{[N]}}L(\\hat{y},y)=\\frac{\\partial L(\\hat{y},y)}{\\partial z^{[N]}}$\n",
    "\n",
    "\n",
    "For sigmoid function, this would be computed with the chain rule:\n",
    "\n",
    "$\\bigtriangledown_{z^{[N]}}L(\\hat{y},y)=\\bigtriangledown_{\\hat{y}}L(\\hat{y},y)\\circ (g^{[N]})'(z^{[N]})$\n",
    "\n",
    "**Step 2**\n",
    "\n",
    "For $l= N-1, N-2,...,1$, we have:\n",
    "\n",
    "$\\delta^{[l]} = (W^{[l+1]T}\\delta^{[l+1]})\\circ g'(z^{[l]})$\n",
    "\n",
    "For the sigmoid function, $g'(z^{[l]})=a^{[l]} \\circ (1 - a^{[l]})=g(z^{[l]})*(1-g(z^{[l]}))$\n",
    "\n",
    "Note that the gradient from the previous layer is used to compute the gradient for the current layer (going backwards)\n",
    "\n",
    "**Step 3**\n",
    "\n",
    "Compute the gradients at layer $l$ as:\n",
    "\n",
    "$\\bigtriangledown_{W^{[l]}}J(W,B) = \\delta^{[l]}a^{[l-1]T}$\n",
    "\n",
    "$\\bigtriangledown_{b^{[l]}}J(W,B) = \\delta^{[l]}a^{[l-1]T}$\n",
    "\n",
    "**Completing the updates**\n",
    "\n",
    "For final layer, the updates will look as follows (assuming 1 training example is used for update):\n",
    "\n",
    "$\\frac{\\partial L(\\hat{y},y)}{\\partial W^{[4]}} = (a^{[4]} - y)a^{[3]T}$\n",
    "\n",
    "$\\frac{\\partial L(\\hat{y},y)}{\\partial b^{[4]}} = (a^{[4]} - y)$\n",
    "\n",
    "$W^{[4]} = W^{[4]} - \\alpha \\frac{\\partial L(\\hat{y},y)}{\\partial W^{[4]}}$\n",
    "\n",
    "$b^{[4]} = b^{[4]} - \\alpha \\frac{\\partial L(\\hat{y},y)}{\\partial b^{[4]}}$\n",
    "\n",
    "Assuming use of sigmoid function, 3rd layer update will be as follows:\n",
    "\n",
    "\n",
    "$\\frac{\\partial L(\\hat{y},y)}{\\partial W^{[3]}} = (a^{[3]} - y)W^{[3]}g(z^{[3]})(1-g(z^{[3]})a^{[2]T}$\n",
    "\n",
    "$\\frac{\\partial L(\\hat{y},y)}{\\partial b^{[3]}} = (a^{[3]} - y)W^{[3]}g(z^{[3]})(1-g(z^{[3]})$\n",
    "\n",
    "$W^{[3]} = W^{[3]} - \\alpha \\frac{\\partial L(\\hat{y},y)}{\\partial W^{[3]}}$\n",
    "\n",
    "$b^{[3]} = b^{[3]} - \\alpha \\frac{\\partial L(\\hat{y},y)}{\\partial b^{[3]}}$\n",
    "\n",
    "#### Stochastic Gradient Descent (SGD)\n",
    "\n",
    "Instead of doing updates using entire training set, you update one training example at a time. This is how it is done above.\n",
    "\n",
    "#### Mini-batch Gradient Descent\n",
    "\n",
    "Use a fixed batch size for updates. For all batches b with size B:\n",
    "\n",
    "$W^{[l]} = W^{[l]} - \\alpha \\Sigma^{B}_{i=1} \\frac{\\partial L(\\hat{y}^{(ib)},y^{(ib)})}{\\partial W^{[l]}}$\n",
    "\n",
    "$b^{[l]} = b^{[l]} - \\alpha \\Sigma^{B}_{i=1} \\frac{\\partial L(\\hat{y}^{(ib)},y^{(ib)})}{\\partial b^{[l]}}$\n",
    "\n",
    "\n",
    "#### Regularization/Dropout\n",
    "\n",
    "L2 regularization can be applied to backprop. Dropout can also be applied. Dropout is esesntially giving each neuron a probability of being kept during training time. The thoery is that this makes it so the algorithm cannot be overly reliant on any one neuron. At test time, all neurons are always present.\n",
    "\n",
    "\n",
    "<img src=\"images/dropout-1.png\" style=\"width:400px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Networks \"RNN\"\n",
    "\n",
    "RNNs are typically used for procesing sequences. They can be used in many different configurations:\n",
    "\n",
    "<img src=\"images/rnn-1.png\" style=\"width:400px\"/>\n",
    "\n",
    "New state is determined by a function using weights on the old state, and a new input vector:\n",
    "\n",
    "$h_{t} = f_{W}(h_{t-1},x_{t})$\n",
    "\n",
    "<img src=\"images/rnn-2.png\" style=\"width:400px\"/>\n",
    "\n",
    "#### Long Short Term Memorty \"LSTM\"\n",
    "\n",
    "Has a gate function on top of recurrent weights to determine if the new state should be output or if the old state should be kept. Useful for passing on information a long way.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA\n",
    "\n",
    "Orthogonal projection of data onto a lower dimensional linear space that:\n",
    "\n",
    "* maximizes variance of projected dta\n",
    "* minimizes mean squared distance between data points and projections\n",
    "\n",
    "<img src=\"images/pca-1.png\" style=\"width:400px\"/>\n",
    "\n",
    "#### The Principal Components\n",
    "\n",
    "* Vectors originating from the center of mass\n",
    "* 1st principal component points in direction of largest variance\n",
    "* Each subsequent principal component is orthogonal to the previous ones and points in the direction of the largest variance of the residual subspace.\n",
    "\n",
    "#### Eigenvectors\n",
    "\n",
    "Let A be a matrix, and consider $Ax = \\lambda x$\n",
    "\n",
    "Eigenvalue of matrix: $\\lambda$\n",
    "Eigenvector: x for which $Ax = \\lambda x$\n",
    "\n",
    "First principal component is the largest eigenvector of covariance matrix $\\Sigma = X^{T}X$\n",
    "Second principal component is orthogonal fo first and second eigenvector of $\\Sigma$\n",
    "\n",
    "PCA can be used for dimensionality reduction. You do lose some information, but not all. \n",
    "\n",
    "<img src=\"images/pca-2.png\" style=\"width:500px\"/>\n",
    "\n",
    "Main drawback: Resulting projected dataset is not interpretable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoders\n",
    "\n",
    "Unsupervised approach for learning a lower-dimensional feature representation of unlabeled training data. Essentially, train a model to go from original data to lower feature representation back to original data.\n",
    "\n",
    "<img src=\"images/auto-1.png\" style=\"width:400px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "\n",
    "Goal: Automatically segment data into groups of similar points without knowing labels.\n",
    "\n",
    "#### K means clustering\n",
    "\n",
    "* Fix a number of desired clusters k\n",
    "* Select k cluster means at random\n",
    "* Assign points to the closest cluster mean\n",
    "* Re-compute cluster means based on new assignment\n",
    "* Continue to do this until convergence.\n",
    "\n",
    "<img src=\"images/k-means-1.png\" style=\"width:400px\"/>\n",
    "<img src=\"images/k-means-2.png\" style=\"width:400px\"/>\n",
    "<img src=\"images/k-means-3.png\" style=\"width:400px\"/>\n",
    "\n",
    "##### Choosing the number of clusters\n",
    "\n",
    "Choosing k is a difficult problem. How to do so depends on the problem.\n",
    "\n",
    "#### Hierarchical clustering\n",
    "\n",
    "Build a binary tree of the data that successively merges similar groups of points\n",
    "\n",
    "Agglomerative clustering algorithm: Place each data point in its own singleton cluster, repeatedly merge the two closest clusters until a stopping condition is satisfied."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
