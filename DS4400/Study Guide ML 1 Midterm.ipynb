{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "* [Probability and Linear Algebra](#Probability-and-Linear-Algebra)\n",
    "    * [Random Variables](#Random-Variables)\n",
    "    * [Axioms/Theorems of Probability/Set Theory](#Axioms/Theorems-of-Probability/Set-Theory)\n",
    "    * [Discrete Random Variables](#Discrete-Random-Variables)\n",
    "    * [Continuous Random Variables](#Continuous-Random-Variables)\n",
    "    * [Expectation and Variance](#Expectation-and-Variance)\n",
    "    * [Conditional Probability and Bayes Theorem](#Conditional-Probability-and-Bayes-Theorem)\n",
    "    * [Vectors and Matrices](#Vectors-and-Matrices)\n",
    "    * [Matrix Products](#Matrix-Products)\n",
    "    * [Matrix Properties](#Matrix-Properties)\n",
    "    * [Linear Independence](#Linear-Independence)\n",
    "* [Linear Regression](#Linear-Regression)\n",
    "    * [Simple LR](#Simple-LR)\n",
    "    * [Terminology and Metrics](#Terminology-and-Metrics)\n",
    "    * [Closed form solution to simple LR](#Closed-form-solution-to-simple-LR)\n",
    "    * [Multiple LR](#Multiple-LR)\n",
    "    * [Closed form solution to multiple LR](#Closed-form-solution-to-multiple-LR)\n",
    "    * [Feature Standardization](#Feature-Standardization)\n",
    "    * [Categorical Variables](#Categorical-Variables)\n",
    "    * [Gradient Descent](#Gradient-Descent)\n",
    "    * [GD for Linear Regression](#GD-for-Linear-Regression)\n",
    "* [Perceptrons](#Perceptrons)\n",
    "    * [Online Learning vs Batch Learning](#Online-Learning-vs-Batch-Learning)\n",
    "    * [Perceptron Limitations and Improvements](#Perceptron-Limitations-and-Improvements)\n",
    "* [KNN](#KNN)\n",
    "* [Cross Validation](#Cross-Validation)\n",
    "    * [K-fold CV](#K-fold-CV)\n",
    "* [Classification Metrics](#Classification-Metrics)\n",
    "    * [Confusion Matrix](#Confusion-Matrix)\n",
    "* [Logistic Regression](#Logistic-Regression)\n",
    "    * [Maximum Likelihood Estimation](#Maximum-Likelihood-Estimation)\n",
    "    * [Gradient Descent for Logistic Regression](#Gradient-Descent-for-Logistic-Regression)\n",
    "* [LDA](#LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability and Linear Algebra\n",
    "\n",
    "#### Random Variables\n",
    "\n",
    "A random variable $A$ represents an event that can take place.\n",
    "\n",
    "*Example*\n",
    "\n",
    "* $A$ = I have a headache\n",
    "* $A$ = Sally will be president in 2020\n",
    "\n",
    "$P(A)$ is the probability $A$ will be true.\n",
    "\n",
    "#### Axioms/Theorems of Probability/Set Theory\n",
    "$P(A \\lor B) = P(A) + P(B) - P(A \\land B)$\n",
    "\n",
    "$P(A \\lor B) \\leq P(A) + P(B)$\n",
    "\n",
    "$P(\\lnot A) = 1 - P(A)$\n",
    "\n",
    "$P(A) = P(A \\land B) + P(A \\land \\lnot B)$\n",
    "\n",
    "#### Discrete Random Variables\n",
    "\n",
    "Discrete random variables (DRV) take on a finite number of values. Uniform random variables are discrete random variables in which each possibility has an equal probability.\n",
    "\n",
    "Bernouli random variables are DRV where there are 2 possibilities.\n",
    "\n",
    "Binomial random variables are DRV where we want to find the probability that a Bernouli random variable comes up k times. To find this, use the following formula (assume p is the probability of the positive case):\n",
    "\n",
    "${n\\choose k}p^{k}(1 - p)^{n - k}$\n",
    "\n",
    "#### Continuous Random Variables\n",
    "\n",
    "$X$ is a continuous random variable (CRV) if $X$ can take on an infinite number of values.\n",
    "\n",
    "The **cumulative distribution function CDF** $F$ for $X$ is defined for every value $x$ by:\n",
    "\n",
    "$F(x) = Pr(X \\leq x)$\n",
    "\n",
    "The **probability distribution function PDF** $f(x)$ for $X$ is\n",
    "\n",
    "$f(x) = \\frac{dF(x)}{dx}$\n",
    "\n",
    "Think of PDF as probability at a point, and CDF of probability that variable is at least that point.\n",
    "\n",
    "#### Expectation and Variance\n",
    "\n",
    "Expectation: The weighted average value for a random variable.\n",
    "\n",
    "*Properties*:\n",
    "* $E[ag(X)] = aE[g(X)] \\text{ (a is constant)}$\n",
    "* $E[f(X) + g(X)] = E[f(X)] + E[g(X)]$\n",
    "\n",
    "Variance: The average value of the square distance from the mean value. Can be calculated as follows:\n",
    "\n",
    "$E[X^{2}] - E[X]^{2}$\n",
    "\n",
    "Here is a good video for finding these values for CRV: https://youtu.be/Ro7dayHU5DQ\n",
    "\n",
    "#### Conditional Probability and Bayes Theorem\n",
    "\n",
    "\n",
    "### $P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$\n",
    "\n",
    "#### Vectors and Matrices\n",
    "\n",
    "Vectors are ordered sets of numbers. Row vectors are of dimensions $n\\times 1$, column vectors are of dimensions $1\\times n$\n",
    "\n",
    "Norms are a measure of the \"length\" of a vector.\n",
    "\n",
    "* L1 norm: $||x||_{1} = \\sum_{n}^{i=1}|x_{i}|$\n",
    "* L2 norm: $||x||_{2} = \\sqrt{\\sum_{n}^{i=1}x_{i}^{2}}$\n",
    "\n",
    "#### Matrix Products\n",
    "\n",
    "Vector dot (inner) product: let $r$ be a row vector, $c$ be a column vector $rc = \\sum_{n}^{i=1}r_{i}c_{i}$\n",
    "\n",
    "In order to multiply matrices, their dimensions must match up: $A \\in I\\!R^{m\\times n} B \\in I\\!R^{n\\times p}$\n",
    "\n",
    "#### Matrix Properties\n",
    "\n",
    "Associative: $(AB)C = A(BC)$\n",
    "\n",
    "Distributive: $A(B + C) = AB + AC$\n",
    "\n",
    "**NOT** Commutative: $AB \\neq BA$\n",
    "\n",
    "Transpose: Think of it as flipping a matrix. A row vector transpose is a column vector. $m\\times n$ &rarr; $n \\times m$\n",
    "\n",
    "#### Linear Independence\n",
    "\n",
    "A set of vectors are linearly independent if none of them can be written as a slinear combination of the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "One of the most widely used ML techniques.\n",
    "\n",
    "#### Simple LR\n",
    "\n",
    "$h_{\\theta}(x) = \\theta_{0} + \\theta_{1}x$\n",
    "\n",
    "In this case, x is single dimensional. This is essentially finding the \"best fit line\".\n",
    "\n",
    "Cost function: $J(\\theta) = \\frac{1}{n}\\sum^{n}_{i=1}(h_{\\theta}(x^{(i)} - y^{(i)})^{2}$ - This is the Mean Square Error (MSE)\n",
    "\n",
    "#### Terminology and Metrics\n",
    "\n",
    "Residuals - Difference between predictions and actual values.\n",
    "\n",
    "$R^{(i)} = |y^{(i)} - \\hat{y}^{(i)}|\\text{, }\\hat{y}^{(i)} = h_{\\theta}(x^{(i)})$\n",
    "\n",
    "Residual Sum of Squares (RSS)\n",
    "\n",
    "$RSS = \\sum[y^{(i)} - \\hat{y}^{(i)}]^{2} $\n",
    "\n",
    "Residual Standard Error (RSE)\n",
    "\n",
    "$RSE = \\sqrt{\\frac{RSS}{n - 2}}$\n",
    "\n",
    "#### Closed form solution to simple LR\n",
    "\n",
    "$\\theta_{0} = \\bar{y} - \\theta_{1}\\bar{x}$\n",
    "\n",
    "$\\theta_{1} = \\frac{\\sum(x^{(i)} - \\bar{x})(y^{(i)} - \\bar{y})}{\\sum(x^{(i)} - \\bar{x})^{2}}$\n",
    "\n",
    "$\\bar{x},\\bar{y} = \\text{mean of x, y respectively}$\n",
    "\n",
    "This was found by taking the partial derivative of the loss function and setting it = 0.\n",
    "\n",
    "\n",
    "#### Multiple LR\n",
    "\n",
    "$h_{\\theta}(x) = \\theta^{T}x$\n",
    "\n",
    "$J(\\theta) = \\frac{1}{n}\\sum^{n}_{i = 1}(h_{\\theta}(x^{(i)}) - y^{(i)})^{2}$\n",
    "\n",
    "#### Closed form solution to multiple LR\n",
    "\n",
    "$\\theta = (X^{T}X)^{-1}X^{T}y$\n",
    "\n",
    "$X$ is the matrix where each row is the number 1 followed by $x_{1}^{(i)}:x_{d}^{(i)}$\n",
    "\n",
    "#### Feature Standardization\n",
    "\n",
    "$\\mu_{j},s_{j} = \\text{mean, standard deviation of feature j}$\n",
    "\n",
    "$x_{j}^{(i)} \\leftarrow \\frac{x_{j}^{(i)} - \\mu_{j}}{s_{j}}$\n",
    "\n",
    "This rescales features to have 0 mean and 1 variance. This is good because different features can be on very different scales, making LR not work as well.\n",
    "\n",
    "#### Categorical Variables\n",
    "\n",
    "Example: State - has 50 values. To encode, we create 49 indicator variables.\n",
    "\n",
    "$x_{MA} = 1 \\text{ if State = MA and 0 otherwise}$\n",
    "\n",
    "This can make it so that data becomes too sparse.\n",
    "\n",
    "#### Gradient Descent\n",
    "\n",
    "Repeat the following:\n",
    "\n",
    "$\\theta_{j} \\leftarrow \\theta_{j} - \\alpha\\frac{\\partial}{\\partial\\theta_{j}}J(\\theta)$\n",
    "\n",
    "Intuition: The derivative indicates whether theta at this point is too high or too low as it indicates the slope at this point, and this will converge to the minimum.\n",
    "\n",
    "#### GD for Linear Regression\n",
    "\n",
    "$\\theta \\leftarrow \\theta - \\alpha \\sum^{n}_{i=1}(h_{\\theta}(x^{(i)}) - y^{(i)})x^{(i)}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptrons\n",
    "\n",
    "Classification algorithm where y=1 for the positive case, y=-1 for the negative case.\n",
    "\n",
    "$h(x) = sign(\\theta^{T}x)$\n",
    "\n",
    "Perceptron update rule:\n",
    "\n",
    "$\\theta_{j} \\leftarrow \\theta_{j} - \\frac{1}{2}(h(x^{(i)}) - y^{(i)})x_{j}^{(i)}$\n",
    "\n",
    "If $x^{(i)}$ is misclassified, do the following: $\\theta \\leftarrow \\theta + y^{(i)}x^{(i)}$\n",
    "\n",
    "#### Online Learning vs Batch Learning\n",
    "\n",
    "Online Learning - Model update is performed after every observation.\n",
    "\n",
    "Batch Learning - Model update is performed on entire training set. Updates are performed by computing the average update, and then updating theta with that.\n",
    "\n",
    "#### Perceptron Limitations and Improvements\n",
    "\n",
    "<p style=\"text-decoration: underline\">Limitations<p>\n",
    "* Dependent on starting point\n",
    "* Could take a long time to converge\n",
    "* Can overfit data\n",
    "* Many different decision boundaries are possible\n",
    "\n",
    "<p style=\"text-decoration: underline\">Improvements<p>\n",
    "\n",
    "* Use a combination of multiple Perceptrons\n",
    "* Averaged Perceptron: Average intermediate perceptrons\n",
    "\n",
    "### KNN\n",
    "\n",
    "Find the K nearest samples in the training data to a test point, classify by majority vote.\n",
    "\n",
    "Typical metric to determine distance is euclidean distance: $\\sqrt{(\\sum^{k}_{i=1}(x_{i} - y_{i})^{2})}$\n",
    "\n",
    "Cons:\n",
    "\n",
    "* Does not learn any model\n",
    "* Instance learner - needs all data at test time.\n",
    "\n",
    "k=1 &rarr; overfit\n",
    "k=n &rarr; potential underfit, takes much longer to classify\n",
    "\n",
    "Choose k through cross validation.\n",
    "\n",
    "### Cross Validation\n",
    "* Split training into training and validation data.\n",
    "* Hold out validation data from training, and measure error with it. This way you test on data the algorithm hasn't seen before.\n",
    "\n",
    "#### K-fold CV\n",
    "\n",
    "Split data into k partitions of equal size, train model with same hyper perameters on each partition, and test on each partition. Each partition has a train and val set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Metrics\n",
    "\n",
    "$accuracy = \\frac{\\text{# correct preds}}{\\text{# total instances}}$\n",
    "\n",
    "$error = 1 - accuracy$\n",
    "\n",
    "#### Confusion Matrix\n",
    "\n",
    "<p style='text-align: center'>Predicted Class</p>\n",
    "\n",
    "|             |     | Yes | No |\n",
    "|-------------|-----|-----|----|\n",
    "|Actual Class | Yes | TP  | FN |\n",
    "|             | No  | FP  | TN |\n",
    "\n",
    "$precision = \\frac{TP}{TP + FP}$ Precision is a measure of how few false positives we had.\n",
    "\n",
    "$recall = \\frac{TP}{TP + FN}$ Recall is a measure of how few false negatives we had.\n",
    "\n",
    "\n",
    "### Logistic Regression\n",
    "\n",
    "$h_{\\theta}(x) = g(\\theta^{T}x)$\n",
    "\n",
    "$g(z) = \\frac{1}{1 + e^{-z}}$ This is known as the sigmoid function.\n",
    "\n",
    "The sigmoid function has the nice property of always being between 0 and 1. This allows logistic regression to be a probabalistic classifier, where the higher $h(x)$ is, the more confident it is.\n",
    "\n",
    "To make predictions we use a threshold. If $h(x)$ > threshold, we predict 1, else we predict 0.\n",
    "\n",
    "#### Maximum Likelihood Estimation\n",
    "\n",
    "What is the likelihood of training data for parameter $\\theta$?\n",
    "\n",
    "$Max_{\\theta}L(\\theta) = P[Y|X;\\theta]$\n",
    "\n",
    "Assumption: training points are independent\n",
    "\n",
    "$L(\\theta) = \\Pi^{n}_{i=1}P[y^{(i)}|x^{(i)};\\theta]$\n",
    "\n",
    "Max likelihood is equivalent to maximizing the log of the likelihood (log likelihood)\n",
    "\n",
    "$log L(\\theta) = \\sum^{n}_{i=1}log P[y^{(i)}|x^{(i)};\\theta]$\n",
    "\n",
    "Expand right side\n",
    "\n",
    "$\\theta_{MLE} = argmax_{\\theta}\\sum^{n}_{i=1}y^{(i)}log(h_{\\theta}(x^{(i)})) + (1 - y^{(i)}) log(1 - h_{\\theta}(x^{(i)}))$\n",
    "\n",
    "Make this negative for the cost function, as we want to minimize it.\n",
    "\n",
    "$J(\\theta) = -\\sum^{n}_{i=1}y^{(i)}log(h_{\\theta}(x^{(i)})) + (1 - y^{(i)}) log(1 - h_{\\theta}(x^{(i)}))$\n",
    "\n",
    "**intuition**: If y = 1, then the second term of cost = 0. If y = 0, the first term = 0.\n",
    "\n",
    "#### Gradient Descent for Logistic Regression\n",
    "\n",
    "$\\theta \\leftarrow \\theta - \\alpha \\sum^{n}_{i=1}(h_{\\theta}(x^{(i)}) - y^{(i)})x^{(i)}$\n",
    "\n",
    "Same as linear regression! Except h is different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
