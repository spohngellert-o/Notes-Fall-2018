{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "* [Probability and Linear Algebra](#Probability-and-Linear-Algebra)\n",
    "    * [Random Variables](#Random-Variables)\n",
    "    * [Axioms/Theorems of Probability/Set Theory](#Axioms/Theorems-of-Probability/Set-Theory)\n",
    "    * [Discrete Random Variables](#Discrete-Random-Variables)\n",
    "    * [Continuous Random Variables](#Continuous-Random-Variables)\n",
    "    * [Expectation and Variance](#Expectation-and-Variance)\n",
    "    * [Conditional Probability and Bayes Theorem](#Conditional-Probability-and-Bayes-Theorem)\n",
    "    * [Vectors and Matrices](#Vectors-and-Matrices)\n",
    "    * [Matrix Products](#Matrix-Products)\n",
    "    * [Matrix Properties](#Matrix-Properties)\n",
    "    * [Linear Independence](#Linear-Independence)\n",
    "* [Linear Regression](#Linear-Regression)\n",
    "    * [Simple LR](#Simple-LR)\n",
    "    * [Terminology and Metrics](#Terminology-and-Metrics)\n",
    "    * [Closed form solution to simple LR](#Closed-form-solution-to-simple-LR)\n",
    "    * [Multiple LR](#Multiple-LR)\n",
    "    * [Closed form solution to multiple LR](#Closed-form-solution-to-multiple-LR)\n",
    "    * [Feature Standardization](#Feature-Standardization)\n",
    "    * [Categorical Variables](#Categorical-Variables)\n",
    "    * [Gradient Descent](#Gradient-Descent)\n",
    "    * [GD for Linear Regression](#GD-for-Linear-Regression)\n",
    "* [Bias-Variance Tradeoff](#Bias-Variance-Tradeoff)\n",
    "* [Regularization](#Regularization)\n",
    "    * [Ridge Regression](#Ridge-Regression)\n",
    "    * [Lasso Regression](#Lasso-Regression)\n",
    "* [Perceptrons](#Perceptrons)\n",
    "    * [Online Learning vs Batch Learning](#Online-Learning-vs-Batch-Learning)\n",
    "    * [Perceptron Limitations and Improvements](#Perceptron-Limitations-and-Improvements)\n",
    "* [KNN](#KNN)\n",
    "* [Cross Validation](#Cross-Validation)\n",
    "    * [K-fold CV](#K-fold-CV)\n",
    "* [Classification Metrics](#Classification-Metrics)\n",
    "    * [Confusion Matrix](#Confusion-Matrix)\n",
    "* [Logistic Regression](#Logistic-Regression)\n",
    "    * [Maximum Likelihood Estimation](#Maximum-Likelihood-Estimation)\n",
    "    * [Gradient Descent for Logistic Regression](#Gradient-Descent-for-Logistic-Regression)\n",
    "* [LDA](#LDA)\n",
    "* [ROC Curves](#ROC-Curves)\n",
    "* [Feature Selection](#Feature-Selection)\n",
    "    * [Wrappers](#Wrappers)\n",
    "    * [Forward Selection](#Forward-Selection)\n",
    "    * [Filters](#Filters)\n",
    "    * [L1 Regularization](#L1-Regularization)\n",
    "* [Decision Trees](#Decision-Trees)\n",
    "    * [Entropy and Information Gain](#Entropy-and-Information-Gain)\n",
    "    * [Learning Decision Trees ID3](#Learning-Decision-Trees-ID3)\n",
    "    * [DT Overfitting and Pruning](#DT-Overfitting-and-Pruning)\n",
    "    * [Threshold Splits](#Threshold-Splits)\n",
    "* [Ensemble Learning](#Ensemble-Learning)\n",
    "    * [Bagging](#Bagging)\n",
    "    * [Random Forest](#Random-Forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability and Linear Algebra\n",
    "\n",
    "#### Random Variables\n",
    "\n",
    "A random variable $A$ represents an event that can take place.\n",
    "\n",
    "*Example*\n",
    "\n",
    "* $A$ = I have a headache\n",
    "* $A$ = Sally will be president in 2020\n",
    "\n",
    "$P(A)$ is the probability $A$ will be true.\n",
    "\n",
    "#### Axioms/Theorems of Probability/Set Theory\n",
    "$P(A \\lor B) = P(A) + P(B) - P(A \\land B)$\n",
    "\n",
    "$P(A \\lor B) \\leq P(A) + P(B)$\n",
    "\n",
    "$P(\\lnot A) = 1 - P(A)$\n",
    "\n",
    "$P(A) = P(A \\land B) + P(A \\land \\lnot B)$\n",
    "\n",
    "#### Discrete Random Variables\n",
    "\n",
    "Discrete random variables (DRV) take on a finite number of values. Uniform random variables are discrete random variables in which each possibility has an equal probability.\n",
    "\n",
    "Bernouli random variables are DRV where there are 2 possibilities.\n",
    "\n",
    "Binomial random variables are DRV where we want to find the probability that a Bernouli random variable comes up k times. To find this, use the following formula (assume p is the probability of the positive case):\n",
    "\n",
    "${n\\choose k}p^{k}(1 - p)^{n - k}$\n",
    "\n",
    "#### Continuous Random Variables\n",
    "\n",
    "$X$ is a continuous random variable (CRV) if $X$ can take on an infinite number of values.\n",
    "\n",
    "The **cumulative distribution function CDF** $F$ for $X$ is defined for every value $x$ by:\n",
    "\n",
    "$F(x) = Pr(X \\leq x)$\n",
    "\n",
    "The **probability distribution function PDF** $f(x)$ for $X$ is\n",
    "\n",
    "$f(x) = \\frac{dF(x)}{dx}$\n",
    "\n",
    "Think of PDF as probability at a point, and CDF of probability that variable is at least that point.\n",
    "\n",
    "#### Expectation and Variance\n",
    "\n",
    "Expectation: The weighted average value for a random variable.\n",
    "\n",
    "*Properties*:\n",
    "* $E[ag(X)] = aE[g(X)] \\text{ (a is constant)}$\n",
    "* $E[f(X) + g(X)] = E[f(X)] + E[g(X)]$\n",
    "\n",
    "Variance: The average value of the square distance from the mean value. Can be calculated as follows:\n",
    "\n",
    "$E[X^{2}] - E[X]^{2}$\n",
    "\n",
    "Here is a good video for finding these values for CRV: https://youtu.be/Ro7dayHU5DQ\n",
    "\n",
    "#### Conditional Probability and Bayes Theorem\n",
    "\n",
    "\n",
    "### $P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$\n",
    "\n",
    "#### Vectors and Matrices\n",
    "\n",
    "Vectors are ordered sets of numbers. Row vectors are of dimensions $n\\times 1$, column vectors are of dimensions $1\\times n$\n",
    "\n",
    "Norms are a measure of the \"length\" of a vector.\n",
    "\n",
    "* L1 norm: $||x||_{1} = \\sum_{n}^{i=1}|x_{i}|$\n",
    "* L2 norm: $||x||_{2} = \\sqrt{\\sum_{n}^{i=1}x_{i}^{2}}$\n",
    "\n",
    "#### Matrix Products\n",
    "\n",
    "Vector dot (inner) product: let $r$ be a row vector, $c$ be a column vector $rc = \\sum_{n}^{i=1}r_{i}c_{i}$\n",
    "\n",
    "In order to multiply matrices, their dimensions must match up: $A \\in I\\!R^{m\\times n} B \\in I\\!R^{n\\times p}$\n",
    "\n",
    "#### Matrix Properties\n",
    "\n",
    "Associative: $(AB)C = A(BC)$\n",
    "\n",
    "Distributive: $A(B + C) = AB + AC$\n",
    "\n",
    "**NOT** Commutative: $AB \\neq BA$\n",
    "\n",
    "Transpose: Think of it as flipping a matrix. A row vector transpose is a column vector. $m\\times n$ &rarr; $n \\times m$\n",
    "\n",
    "#### Linear Independence\n",
    "\n",
    "A set of vectors are linearly independent if none of them can be written as a slinear combination of the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "One of the most widely used ML techniques.\n",
    "\n",
    "#### Simple LR\n",
    "\n",
    "$h_{\\theta}(x) = \\theta_{0} + \\theta_{1}x$\n",
    "\n",
    "In this case, x is single dimensional. This is essentially finding the \"best fit line\".\n",
    "\n",
    "Cost function: $J(\\theta) = \\frac{1}{n}\\sum^{n}_{i=1}(h_{\\theta}(x^{(i)} - y^{(i)})^{2}$ - This is the Mean Square Error (MSE)\n",
    "\n",
    "#### Terminology and Metrics\n",
    "\n",
    "Residuals - Difference between predictions and actual values.\n",
    "\n",
    "$R^{(i)} = |y^{(i)} - \\hat{y}^{(i)}|\\text{, }\\hat{y}^{(i)} = h_{\\theta}(x^{(i)})$\n",
    "\n",
    "Residual Sum of Squares (RSS)\n",
    "\n",
    "$RSS = \\sum[y^{(i)} - \\hat{y}^{(i)}]^{2} $\n",
    "\n",
    "Residual Standard Error (RSE)\n",
    "\n",
    "$RSE = \\sqrt{\\frac{RSS}{n - 2}}$\n",
    "\n",
    "#### Closed form solution to simple LR\n",
    "\n",
    "$\\theta_{0} = \\bar{y} - \\theta_{1}\\bar{x}$\n",
    "\n",
    "$\\theta_{1} = \\frac{\\sum(x^{(i)} - \\bar{x})(y^{(i)} - \\bar{y})}{\\sum(x^{(i)} - \\bar{x})^{2}}$\n",
    "\n",
    "$\\bar{x},\\bar{y} = \\text{mean of x, y respectively}$\n",
    "\n",
    "This was found by taking the partial derivative of the loss function and setting it = 0.\n",
    "\n",
    "\n",
    "#### Multiple LR\n",
    "\n",
    "$h_{\\theta}(x) = \\theta^{T}x$\n",
    "\n",
    "$J(\\theta) = \\frac{1}{n}\\sum^{n}_{i = 1}(h_{\\theta}(x^{(i)}) - y^{(i)})^{2}$\n",
    "\n",
    "#### Closed form solution to multiple LR\n",
    "\n",
    "$\\theta = (X^{T}X)^{-1}X^{T}y$\n",
    "\n",
    "$X$ is the matrix where each row is the number 1 followed by $x_{1}^{(i)}:x_{d}^{(i)}$\n",
    "\n",
    "#### Feature Standardization\n",
    "\n",
    "$\\mu_{j},s_{j} = \\text{mean, standard deviation of feature j}$\n",
    "\n",
    "$x_{j}^{(i)} \\leftarrow \\frac{x_{j}^{(i)} - \\mu_{j}}{s_{j}}$\n",
    "\n",
    "This rescales features to have 0 mean and 1 variance. This is good because different features can be on very different scales, making LR not work as well.\n",
    "\n",
    "#### Categorical Variables\n",
    "\n",
    "Example: State - has 50 values. To encode, we create 49 indicator variables.\n",
    "\n",
    "$x_{MA} = 1 \\text{ if State = MA and 0 otherwise}$\n",
    "\n",
    "This can make it so that data becomes too sparse.\n",
    "\n",
    "#### Gradient Descent\n",
    "\n",
    "Repeat the following:\n",
    "\n",
    "$\\theta_{j} \\leftarrow \\theta_{j} - \\alpha\\frac{\\partial}{\\partial\\theta_{j}}J(\\theta)$\n",
    "\n",
    "Intuition: The derivative indicates whether theta at this point is too high or too low as it indicates the slope at this point, and this will converge to the minimum.\n",
    "\n",
    "#### GD for Linear Regression\n",
    "\n",
    "$\\theta \\leftarrow \\theta - \\alpha \\sum^{n}_{i=1}(h_{\\theta}(x^{(i)}) - y^{(i)})x^{(i)}$\n",
    "\n",
    "### Bias-Variance Tradeoff\n",
    "\n",
    "Bias - The difference between the estimated and true models (Underfit)\n",
    "Variance - Model difference on different training sets (Overfit)\n",
    "\n",
    "Want optimal model complexity where we have both low bias and low variance. As bias goes up, variance goes down and vice versa.\n",
    "\n",
    "### Regularization\n",
    "\n",
    "We want to penalize large values of $\\theta_{j}$\n",
    "\n",
    "Solution: incorporate this into the cost function.\n",
    "\n",
    "#### Ridge Regression\n",
    "\n",
    "$J(\\theta) = \\frac{1}{2}\\sum^{n}_{i=1}(h_{\\theta}(x^{(i)}) - y^{(i)})^{2} + \\frac{\\lambda}{2}\\sum^{d}_{j=1}\\theta_{j}^{2}$\n",
    "\n",
    "We are using the L2 norm here. Note that $\\theta_{0}$ is not regularized.\n",
    "\n",
    "The higher lambda is, the more regularization. More regularization will create a model with higher bias and lower variance. We can use this to get the optimal model.\n",
    "\n",
    "**Gradient Update**: $\\theta \\leftarrow \\theta - \\alpha \\sum^{n}_{i=1}(h_{\\theta}(x^{(i)}) - y^{(i)})x^{(i)} - \\alpha\\lambda\\theta_{j}$\n",
    "\n",
    "#### Lasso Regression\n",
    "\n",
    "$J(\\theta) = \\frac{1}{2}\\sum^{n}_{i=1}(h_{\\theta}(x^{(i)}) - y^{(i)})^{2} + \\lambda\\sum^{d}_{j=1}|\\theta_{j}|$\n",
    "\n",
    "Uses L1 norm for regularization. There is no closed form solution in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptrons\n",
    "\n",
    "Classification algorithm where y=1 for the positive case, y=-1 for the negative case.\n",
    "\n",
    "$h(x) = sign(\\theta^{T}x)$\n",
    "\n",
    "Perceptron update rule:\n",
    "\n",
    "$\\theta_{j} \\leftarrow \\theta_{j} - \\frac{1}{2}(h(x^{(i)}) - y^{(i)})x_{j}^{(i)}$\n",
    "\n",
    "If $x^{(i)}$ is misclassified, do the following: $\\theta \\leftarrow \\theta + y^{(i)}x^{(i)}$\n",
    "\n",
    "#### Online Learning vs Batch Learning\n",
    "\n",
    "Online Learning - Model update is performed after every observation.\n",
    "\n",
    "Batch Learning - Model update is performed on entire training set. Updates are performed by computing the average update, and then updating theta with that.\n",
    "\n",
    "#### Perceptron Limitations and Improvements\n",
    "\n",
    "<p style=\"text-decoration: underline\">Limitations<p>\n",
    "* Dependent on starting point\n",
    "* Could take a long time to converge\n",
    "* Can overfit data\n",
    "* Many different decision boundaries are possible\n",
    "\n",
    "<p style=\"text-decoration: underline\">Improvements<p>\n",
    "\n",
    "* Use a combination of multiple Perceptrons\n",
    "* Averaged Perceptron: Average intermediate perceptrons\n",
    "\n",
    "### KNN\n",
    "\n",
    "Find the K nearest samples in the training data to a test point, classify by majority vote.\n",
    "\n",
    "Typical metric to determine distance is euclidean distance: $\\sqrt{(\\sum^{k}_{i=1}(x_{i} - y_{i})^{2})}$\n",
    "\n",
    "Cons:\n",
    "\n",
    "* Does not learn any model\n",
    "* Instance learner - needs all data at test time.\n",
    "\n",
    "k=1 &rarr; overfit\n",
    "k=n &rarr; potential underfit, takes much longer to classify\n",
    "\n",
    "Choose k through cross validation.\n",
    "\n",
    "### Cross Validation\n",
    "* Split training into training and validation data.\n",
    "* Hold out validation data from training, and measure error with it. This way you test on data the algorithm hasn't seen before.\n",
    "\n",
    "#### K-fold CV\n",
    "\n",
    "Split data into k partitions of equal size, train model with same hyper perameters on each partition, and test on each partition. Each partition has a train and val set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Metrics\n",
    "\n",
    "$accuracy = \\frac{\\text{# correct preds}}{\\text{# total instances}}$\n",
    "\n",
    "$error = 1 - accuracy$\n",
    "\n",
    "#### Confusion Matrix\n",
    "\n",
    "<p style='text-align: center'>Predicted Class</p>\n",
    "\n",
    "|             |     | Yes | No |\n",
    "|-------------|-----|-----|----|\n",
    "|Actual Class | Yes | TP  | FN |\n",
    "|             | No  | FP  | TN |\n",
    "\n",
    "$precision = \\frac{TP}{TP + FP}$ Precision is a measure of how few false positives we had.\n",
    "\n",
    "$recall = \\frac{TP}{TP + FN}$ Recall is a measure of how few false negatives we had.\n",
    "\n",
    "\n",
    "### Logistic Regression\n",
    "\n",
    "$h_{\\theta}(x) = g(\\theta^{T}x)$\n",
    "\n",
    "$g(z) = \\frac{1}{1 + e^{-z}}$ This is known as the sigmoid function.\n",
    "\n",
    "The sigmoid function has the nice property of always being between 0 and 1. This allows logistic regression to be a probabalistic classifier, where the higher $h(x)$ is, the more confident it is.\n",
    "\n",
    "To make predictions we use a threshold. If $h(x)$ > threshold, we predict 1, else we predict 0.\n",
    "\n",
    "#### Maximum Likelihood Estimation\n",
    "\n",
    "What is the likelihood of training data for parameter $\\theta$?\n",
    "\n",
    "$Max_{\\theta}L(\\theta) = P[Y|X;\\theta]$\n",
    "\n",
    "Assumption: training points are independent\n",
    "\n",
    "$L(\\theta) = \\Pi^{n}_{i=1}P[y^{(i)}|x^{(i)};\\theta]$\n",
    "\n",
    "Max likelihood is equivalent to maximizing the log of the likelihood (log likelihood)\n",
    "\n",
    "$log L(\\theta) = \\sum^{n}_{i=1}log P[y^{(i)}|x^{(i)};\\theta]$\n",
    "\n",
    "Expand right side\n",
    "\n",
    "$\\theta_{MLE} = argmax_{\\theta}\\sum^{n}_{i=1}y^{(i)}log(h_{\\theta}(x^{(i)})) + (1 - y^{(i)}) log(1 - h_{\\theta}(x^{(i)}))$\n",
    "\n",
    "Make this negative for the cost function, as we want to minimize it.\n",
    "\n",
    "$J(\\theta) = -\\sum^{n}_{i=1}y^{(i)}log(h_{\\theta}(x^{(i)})) + (1 - y^{(i)}) log(1 - h_{\\theta}(x^{(i)}))$\n",
    "\n",
    "**intuition**: If y = 1, then the second term of cost = 0. If y = 0, the first term = 0.\n",
    "\n",
    "#### Gradient Descent for Logistic Regression\n",
    "\n",
    "$\\theta \\leftarrow \\theta - \\alpha \\sum^{n}_{i=1}(h_{\\theta}(x^{(i)}) - y^{(i)})x^{(i)}$\n",
    "\n",
    "Same as linear regression! Except h is different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA\n",
    "\n",
    "Classifies to one of k classes using Bayes Theorem\n",
    "\n",
    "Let $\\pi_{k} = P[Y=k]$ be the prior probability of class k, $f_{k}(x) = P[X=x|Y=k]$\n",
    "\n",
    "$Pr(Y=k | X=x) = \\frac{\\pi_{k}f_{k}(x)}{\\sum^{K}_{l=1}\\pi_{l}f_{l}(x)}$\n",
    "\n",
    "Assume $f_{k}(x)$ is gaussian.\n",
    "\n",
    "Steps for LDA in practice:\n",
    "\n",
    "1. Estimate mean and variance on training data.\n",
    "2. Estimate prior $\\hat{\\pi}_{k} = n_{k}/n$\n",
    "3. Given testing point x, predict k (class) that maximizes: $\\delta_{k}(x) = x \\frac{\\hat{\\mu}_{k}}{\\hat{\\sigma}^{2}} - \n",
    "\\frac{\\hat{\\mu}_{k}^{2}}{2\\hat{\\sigma}^{2}} + log(\\hat{\\pi}_{k})$\n",
    "\n",
    "### ROC Curves\n",
    "\n",
    "ROC (Receiver Operator Characteristic)\n",
    "\n",
    "Uses True Postitive Rate (TPR) and False Positive Rate (FPR)\n",
    "\n",
    "$TPR = \\frac{TP}{\\text{All Positives}}, FPR = \\frac{FP}{\\text{All Negatives}}$\n",
    "\n",
    "The ROC curve is generated by graphing the TPR on the y-axis, and the FPR on the x-axis at different thresholds.\n",
    "\n",
    "Area Under Curve (AUC) measures how much area there is under the plotted AUC curve. The closer to 1 the better.\n",
    "\n",
    "### Feature Selection\n",
    "\n",
    "Process of chosing optimal subset of features.\n",
    "\n",
    "#### Wrappers\n",
    "\n",
    "Select features that give best prediction accuracy on cv.\n",
    "\n",
    "Search strategy is either forward selection or backward selection.\n",
    "\n",
    "#### Forward Selection\n",
    "\n",
    "1. Start with empty set of features\n",
    "2. Add each feature individually, train and compute accuracy\n",
    "3. Pick best of those, and keep that feature, train on all other features.\n",
    "4. Repeat 2-3 until all features are added.\n",
    "5. Pick model with best cv accuracy.\n",
    "\n",
    "#### Filters\n",
    "\n",
    "Compute statistics for how well features in train data correlate with response variable (for example correlation coefficient), remove any features that don't reach a certain threshold.\n",
    "\n",
    "Lower computational cost, so very popular.\n",
    "\n",
    "#### L1 Regularization\n",
    "\n",
    "Performs feature selection as part of the learning procedure, but is computationally demanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees\n",
    "\n",
    "Each internal \"node\" tests one attribute of X. Each branch selects, and the leaves are predictions.\n",
    "\n",
    "Decision trees can represent any boolean function, Logistic Regression cannot.\n",
    "\n",
    "To learn a decision tree, we start from an empty tree, split on the next best attribute, and recurse. But how do we pick which feature to split on?\n",
    "\n",
    "#### Entropy and Information Gain\n",
    "\n",
    "Higher difference between true predictions on a split and false predictions mean more information gained. Pick features with highest difference.\n",
    "\n",
    "$H(Y|X)$ = The average conditional entropy of Y\n",
    "\n",
    "For example, if we have the following table of values for 2 features X and Y:\n",
    "\n",
    "|  X      | Y    |   |   |   |\n",
    "|---------|------|---|---|---|\n",
    "| Math    | Yes  |   |   |   |\n",
    "| History | No   |   |   |   |\n",
    "| CS      | Yes  |   |   |   |\n",
    "| Math    | No   |   |   |   |\n",
    "| Math    | No   |   |   |   |\n",
    "| CS      | Yes  |   |   |   |\n",
    "| History | No   |   |   |   |\n",
    "| Math    | Yes  |   |   |   |\n",
    "\n",
    "H(Y|X=Math) = 1 (as you need on average 1 bit to encode Math)\n",
    "H(Y|X=History) = 0 (as it is always \"no\")\n",
    "\n",
    "Information gain (IG) is the following: $IG(Y|X) = H(Y) - H(Y|X)$\n",
    "\n",
    "In this example, $IG(Y|X) = 1 - (0.25 * 0 + 0.25 * 0 + 0.5 * 1) = 0.5$\n",
    "\n",
    "#### Learning Decision Trees ID3\n",
    "\n",
    "* Start from empty tree\n",
    "* Split on next best attribute, based on IG\n",
    "* Recurse\n",
    "\n",
    "Question: When do we stop?\n",
    "\n",
    "Case 1 - Don't split if all matching training data have same output value.\n",
    "\n",
    "Case 2 - Don't split a node if data points are identical on remaining attributes\n",
    "\n",
    "If we recurse forever, we will overfit to training data.\n",
    "\n",
    "#### DT Overfitting and Pruning\n",
    "\n",
    "Solution 1: Pick a fixed depth for the tree\n",
    "\n",
    "Solution 2: Pick a minimum number of samples per leaf\n",
    "\n",
    "Pruning - Train on training data, remove nodes that most improve validation accuracy upon removal.\n",
    "\n",
    "Whole subtrees are replaced by leaf nodes. Subtree is removed if expected error on subtree is higher than that on leaf.\n",
    "\n",
    "#### Threshold Splits\n",
    "\n",
    "Used for numerical features (non-categorical). Split on $x < n$. IG metric can be computed on this as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Learning\n",
    "\n",
    "Construct a classifier H(x) that combines the individual decisions of $h_{1}, ..., h_{l}$\n",
    "\n",
    "The general idea is we train multiple classifiers, and use a vote from those classifiers as the final output.\n",
    "\n",
    "One way to combine is a vote of all classifiers, or a weighted vote.\n",
    "\n",
    "#### Bagging\n",
    "\n",
    "Split training data into k samples. Train k classifiers, one on each of the k samples. Classify a new instance by majority vote from these classifiers.\n",
    "\n",
    "Bagging works very well for decision trees, as DTs have high variance.\n",
    "\n",
    "#### Random Forest\n",
    "\n",
    "Use both bagging, and \"random vector method\", where instead of split on all features, split on $m$ of the features. Generally $m=\\sqrt{p} \\text{, where p = the number of features}$\n",
    "\n",
    "#### Gini Index\n",
    "\n",
    "* Take a node of decision tree\n",
    "* Let $p_{i}$ be the fraction of examples from class i\n",
    "* Measures the \"purity\" of the node\n",
    "    * If node has most examples from one class, Gini index is low\n",
    "* What is the probability that a random node example is mix-classfied at that node?\n",
    "    * $\\sum^{k}_{i=1}p_{i}(1 - p_{i})$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
